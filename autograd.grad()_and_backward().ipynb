{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch import autograd\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is my understanding of autograd.grad() and backward() It could be wrong somewhere.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first create a network and it is: y = ( (w1*x)*w2 + b2 ) ^ 2 \n",
    "# In this example: w1=3,  w2=2,  b2=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(1,1, bias=False)\n",
    "        self.linear1.weight.data = torch.tensor([[3.]])\n",
    "        \n",
    "        self.linear2 = nn.Linear(1,1, bias=True)\n",
    "        self.linear2.weight.data = torch.tensor([[2.]])\n",
    "        self.linear2.bias.data = torch.tensor([6.])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.linear1(x)\n",
    "        return l, self.linear2(l).pow(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a network and define an input (z) as 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "z = torch.tensor([[4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run forward, here we have intermediate output l and final output y\n",
    "# l is actually (w1*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12.]], grad_fn=<MmBackward>),\n",
       " tensor([[900.]], grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l, y = net(z)\n",
    "l, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can verify here that l = (w1*x) = 3*4 = 12\n",
    "# and y = (l*w2+b)^2 = (12*2+6)^2 = 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's call backward. Here we want to calculate grad with respect to l \n",
    "# in other words, if you think l as an input, then we want to know what is its effect \n",
    "# on final output y when it changes 1 (basically that's defination/understanding of gradient)\n",
    "# we can maunally calculate here: y'(l) = 2*(w2)^2*l + 2(b2)(w2). \n",
    "# plug in the values we can have: y'(l) = 2*(2^2)*12 + 2*6*2 = 120 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[120.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad, = autograd.grad( outputs=y, inputs=l )\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPOTRANT \n",
    "# In the above autograd, it looks like equivalent to call backward() (Of course, since inputs is l,\n",
    "# backward only needs to happend to l, which means w1 does not need to be reached. But I am not sure\n",
    "# whether internally they calculate w1 grad, but this is not important). However, autograd.grad() is \n",
    "# not same as calling backward().  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you call backward() to a tensor (usually loss), Pytorch will calculate and POPULATE grad to \n",
    "# all leaf tensors used for calculting this tensor. By leaf tensor, in pytorch, it means 'input'\n",
    "# and 'weight'. In the above example x w1 w2 b2 are leaf tensors. l is not, it is just an intermediate result.\n",
    "# After getting grad, you can use optimizer to do update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a new net and forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "z = torch.tensor([[4.]])\n",
    "l, y = net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call backward for f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see grad exists for all leaf nodes \n",
    "# Of course, we did not set z.requires_grad=True, thus z does not have grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[480.]])\n",
      "tensor([[720.]])\n",
      "tensor([60.])\n"
     ]
    }
   ],
   "source": [
    "print(z.grad)\n",
    "print(net.linear1.weight.grad)\n",
    "print(net.linear2.weight.grad)\n",
    "print(net.linear2.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's back to augograd.grad again. The difference for autograd.grad is that: it will \n",
    "# only return grad with respect to input given in augograd.grad(), however, this grad will not \n",
    "# be populated into that tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, let's do it again  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[480.]])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "z = torch.tensor([[4.]])\n",
    "l, y = net(z)\n",
    "grad, = autograd.grad( outputs=y, inputs=net.linear1.weight )\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See, it is same as when you call backward(), however, this grad is not stored in net.linear1.weight tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net.linear1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thus you can not expect to use autograd and then use optimizer.step() to update your weights.\n",
    "# since grad is not populated to any tensors along the way to calculating grad of w1 \n",
    "# and the worse thing here is: like calling backward(), autograd.grad() also needs computational graph, \n",
    "# by default, once you call autograd.grad(), the graph will be freed and, you can not call the autograd() again\n",
    "# or call backward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bd83ba589fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "grad, = autograd.grad( outputs=y, inputs=net.linear2.weight )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ab75bb780f4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thus, if you want to use the same graph, you need to set retain_graph=True. \n",
    "# Let's do it again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[480.]])\n",
      "tensor([[720.]])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "z = torch.tensor([[4.]])\n",
    "l, y = net(z)\n",
    "grad, = autograd.grad( outputs=y, inputs=net.linear1.weight, retain_graph=True )\n",
    "print(grad)\n",
    "grad, = autograd.grad( outputs=y, inputs=net.linear2.weight, retain_graph=True )\n",
    "print(grad)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check grad here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[480.]])\n",
      "tensor([[720.]])\n",
      "tensor([60.])\n"
     ]
    }
   ],
   "source": [
    "print(net.linear1.weight.grad)\n",
    "print(net.linear2.weight.grad)\n",
    "print(net.linear2.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that these three grad is stored when you last call f.backward(), it does not happen when you \n",
    "# call two times autograd.grad before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, what are the benefits of using autograd if it can not populate grad to any leaf tensors?\n",
    "# one benefit is that it can return grad to non-leaf tensor. Just like the very first example \n",
    "# we can calculate grad with respect to l.\n",
    "# another benefit is: it can be used to calculate higher derivative. \n",
    "# Here comes create_graph=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[120.]], grad_fn=<TBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "z = torch.tensor([[4.]])\n",
    "l, y = net(z)\n",
    "f, = autograd.grad( outputs=y, inputs=l, create_graph=True )\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by setting create_graph=True, we create a graph for grad. In other words, \n",
    "# in this graph, the final output is no longer y, but it is grad. \n",
    "# mathematically, y'(l) = 2*(w2)^2*l + 2(b2)(w2) \n",
    "# Thus, this new grah is for above equation, let's define f = y'(l)\n",
    "# f = 2*(w2)^2*l + 2(b2)(w2) = 2*(w2)^2*(w1)x + 2(b2)(w2)\n",
    "# This could be useful in some cases where your loss is related with grad. \n",
    "# For example, in some regulization, you want to minimize grad. \n",
    "# Let's say you want to minimize grad with respect to l, in other words, you want to minimize f \n",
    "# so let's call f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32.]])\n",
      "tensor([[108.]])\n",
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "print(net.linear1.weight.grad)\n",
    "print(net.linear2.weight.grad)\n",
    "print(net.linear2.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and verify them\n",
    "# f'(w1) = 2*(w2)^2*x = 2*2^2*4 = 32\n",
    "# f'(w2) = 4*(w2)*x*(w1) + 2*(b2) = 4*2*4*3 + 2*6 = 108\n",
    "# f'(b2) = 2*(w2) = 2*2 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use autograd to calculate second derivative of w1: y''(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[480.]], grad_fn=<TBackward>)\n",
      "tensor([[128.]])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "z = torch.tensor([[4.]])\n",
    "l, y = net(z)\n",
    "first_derivative, = autograd.grad( outputs=y, inputs=net.linear1.weight, create_graph=True )\n",
    "print(first_derivative)\n",
    "second_derivative, = autograd.grad( outputs=first_derivative, inputs=net.linear1.weight )\n",
    "print(second_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can verify \n",
    "# like before the first derivative is y'(w1) = 2*x^2*(w1)*(w2)^2 + 2*x*w2*b2 = 480\n",
    "# the second derivative is y''(w1) = 2*x^2*(w2)^2 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, grad is accumulated in Pytorch. For example if you call backward to both y and f\n",
    "# then grad will be summed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[512.]])\n",
      "tensor([[828.]])\n",
      "tensor([64.])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "z = torch.tensor([[4.]])\n",
    "l, y = net(z)\n",
    "f, = autograd.grad( outputs=y, inputs=l, create_graph=True )\n",
    "f.backward(retain_graph=True)\n",
    "y.backward()\n",
    "\n",
    "print(net.linear1.weight.grad)\n",
    "print(net.linear2.weight.grad)\n",
    "print(net.linear2.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that 512=32+480; 828=108+720; 64=4+60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One thing buffles me is that: my understanding is that once you call create_graph=True, then there will be \n",
    "# two graphs. When I call f.backward(), if not set retain_graph=True then the second graph will be freed \n",
    "# and the first one is still there, so you should be able to call y.backward(). However, this is not the case\n",
    "# it performances like: there is one graph. If you do not set retain_graph=True in f.backward(), then you can \n",
    "# not call y.backward(). Note that in autograd.grad(), there is also one argument retain_graph,\n",
    "# it is set the same as create_graph. So, if you set retain_graph=False in autograd.grad(),\n",
    "# then you can not call f.backward() either. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
